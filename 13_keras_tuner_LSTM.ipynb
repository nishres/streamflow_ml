{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OMqhbmCUHJKJA8PLPYSq9xLfKwcL0Kpm",
      "authorship_tag": "ABX9TyPGA6eAXcdc2TdckmivLkuc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishres/streamflow_ml/blob/main/13_keras_tuner_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install hydroeval\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtuM_ri3bSLP",
        "outputId": "d63f2089-1f7c-4e1d-d6a1-46ea0575e22d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5k22gT2cAHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a7a0a0-cd51-4d1a-987d-cccd0402f29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-a19654ceb361>:16: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import kerastuner as kt\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#import hydroeval as he\n",
        "#import seaborn as sns\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint    #, ReduceLROnPlateau\n",
        "import keras.backend\n",
        "import kerastuner as kt\n",
        "from google.colab import output\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8TJCMWQs4TFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835275d2-cb5b-43c1-a9e9-3dc87727e6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_val_test2(basin_name):\n",
        "    available_data = {\n",
        "        'Babai_Chepang':[4653,6669,3652,4652,6670,7669],\n",
        "        'Bagmati_Padheradovan':[3652,7000,7001,8945,8949,9731],\n",
        "        #'Balephi_Jalbire':[3652,7100,7101,8745,8746,9495],\n",
        "        'Bhotekoshi_Barabise':[5310,6633,7670,8400,6726,6939],\n",
        "        'Budhigandaki_Arughat':[5654,8714,3652,4657,4658,5647],\n",
        "        'Budhiganga_Chitra':[3823,5650,5651,6251,6252,6939],\n",
        "        'Chamelia_Nayalbadi':[3652,7200,7201,8745,8746,9495],\n",
        "        'Dudhkoshi_Rabuwabazar':[3652,5870,5871,7059,7305,9130],\n",
        "        # 'Indrawati_Dolalghat':[5844,7029,7670,8270,8271,8765],\n",
        "        # 'Kaligandaki_Anshing':[3652,7200,7201,8745,8746,9495],\n",
        "        #'Likhu_Sangutar':[6193,7198,5479,6102,4748,5112],\n",
        "        'Madi_Shisaghat':[3755,7300,7301,8800,8801,9753],\n",
        "        'Marikhola_Nayagaon':[3652,7200,7201,8800,8801,9814],\n",
        "        'Marsyangdi_Bhakundebesi':[3683,4883,5326,6208,4884,5316],\n",
        "        'Modikhola_Nayapul':[5338,8795,3652,5329,8796,9495],\n",
        "        'Myagdikhola_Mangalghat':[6991,9495,3712,4887,6018,6498],\n",
        "        'Rapti_Bagasotigaon':[3652,6600,6601,8045,8046,8936],\n",
        "        'Rapti_Jalkundi':[3652,7300,7301,8800,8801,9861],\n",
        "        # 'Rapti_Kusum':[4748,7600,7601,9145,9146,10122],\n",
        "        # 'Sabhyakhola_Tumlingtar':[7670,9495,6209,7624,5098,6192],\n",
        "        # 'Saradakhola_Daradhunga':[3652,6939,7670,8970,8971,9861],\n",
        "        'Seti_Bangga':[7213,9861,3987,5600,5601,6604],\n",
        "        'Seti_Gopaghat':[3652,7200,7201,8745,8746,9131],\n",
        "        'Setigandaki_Damauli':[3652,7300,7301,8800,8801,9861],\n",
        "        #'Sinjhakhola_Diware':[3652,7200,7201,8745,8746,9495],\n",
        "        #'Sunkoshi_Khurkot':[5113,6458,8766,9495,6459,7258],\n",
        "        'Sunkoshi_Pachwarghat':[3652,7200,7201,8751,8933,9495],\n",
        "        #'Tadi_Belkot':[4337,7500,7501,9495,3652,4309],\n",
        "        'Tamakoshi_Busti':[5113,6939,7670,8400,8766,9495],\n",
        "        'Tamor_Majhitar':[3652,6132,6677,8400,6209,6644],\n",
        "        'Tamor_Mulghat':[4868,7400,7401,8765,4183,4747],\n",
        "        'Thulibheri_Rimna':[3652,6600,6601,8533,8534,9861],\n",
        "        'Tilanadi_Nagma':[3652,6400,8035,9495,6401,7945],\n",
        "        'Trishuli_Betrawati':[3652,7200,7201,8745,8746,9495],\n",
        "\n",
        "    }\n",
        "    for key in available_data.keys():\n",
        "        if key in basin_name:\n",
        "            train_s = available_data[key][0]\n",
        "            train_e = available_data[key][1]\n",
        "            val_s = available_data[key][2]\n",
        "            val_e = available_data[key][3]\n",
        "            test_s = available_data[key][4]\n",
        "            test_e = available_data[key][5]\n",
        "    #print(train_s,train_e,val_s, val_e, test_s, test_e)\n",
        "    return train_s, train_e, val_s, val_e, test_s, test_e"
      ],
      "metadata": {
        "id": "h4a1UAIEozEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_days = [0]\n",
        "#n_days = [ 1, 3, 5, 7, 10 ]                        # number of days lag\n",
        "#drop_rate = [ 0.01 ]\n",
        "drop_rate = [ 0.2, 0.100, 0.010, 0.001 ]         # dropout rate 0.050,\n",
        "#cell_layers = [ 50]\n",
        "l_rate = 0.01                                            # learning rate\n",
        "cells = [ 50 ]"
      ],
      "metadata": {
        "id": "iFFWMC13ZEsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_path = '/content/drive/MyDrive/2_2/00_Raw_Agg'\n",
        "\n",
        "all_files = os.listdir(in_path)\n",
        "for i in range(len(all_files)):\n",
        "  print(i,all_files[i])\n",
        "del i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubxx5Id7K0Vc",
        "outputId": "2571fedc-1bb7-45ed-e5a5-800412a48093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Budhiganga_Chitra_basin_avg.csv\n",
            "1 Chamelia_Nayalbadi_basin_avg.csv\n",
            "2 Dudhkoshi_Rabuwabazar_basin_avg.csv\n",
            "3 Madi_Shisaghat_basin_avg.csv\n",
            "4 Marikhola_Nayagaon_basin_avg.csv\n",
            "5 Marsyangdi_Bhakundebesi_basin_avg.csv\n",
            "6 Modikhola_Nayapul_basin_avg.csv\n",
            "7 Rapti_Bagasotigaon_basin_avg.csv\n",
            "8 Myagdikhola_Mangalghat_basin_avg.csv\n",
            "9 Rapti_Jalkundi_basin_avg.csv\n",
            "10 Setigandaki_Damauli_basin_avg.csv\n",
            "11 Seti_Gopaghat_basin_avg.csv\n",
            "12 Sunkoshi_Pachwarghat_basin_avg.csv\n",
            "13 Tamakoshi_Busti_basin_avg.csv\n",
            "14 Tamor_Majhitar_basin_avg.csv\n",
            "15 Tamor_Mulghat_basin_avg.csv\n",
            "16 Tilanadi_Nagma_basin_avg.csv\n",
            "17 Thulibheri_Rimna_basin_avg.csv\n",
            "18 Trishuli_Betrawati_basin_avg.csv\n",
            "19 Budhigandaki_Arughat_basin_avg.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(b_name,r_df):\n",
        "    r_df = r_df[['t2m_mean','src_mean','snowc_mean','tp_sum','ssro_sum', 'e_sum' ,'sf_sum','Observed']]\n",
        "    train_start, train_end, val_start, val_end, test_start, test_end = get_train_val_test2(b_name)\n",
        "    #print(train_start, train_end, val_start, val_end, test_start, test_end)\n",
        "    df_tr = r_df[train_start:train_end]\n",
        "    df_vl = r_df[val_start:val_end]\n",
        "    df_ts = r_df[test_start:test_end]\n",
        "    return normalize(df_tr), normalize(df_vl), normalize(df_ts)"
      ],
      "metadata": {
        "id": "j_B-4AeU5Ku_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(udf):\n",
        "    #scaler = MinMaxScaler()\n",
        "    scaler = StandardScaler()\n",
        "    udf = udf[[c for c in udf if c not in ['time']]]\n",
        "    #print(udf)\n",
        "    column_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('scaler', scaler, ['t2m_mean', 'src_mean', 'snowc_mean', 'tp_sum','ssro_sum','e_sum' ,'sf_sum'])\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    transformed_data = column_transformer.fit_transform(udf)\n",
        "    df_normalized = pd.DataFrame(transformed_data, columns=udf.columns)\n",
        "    del udf\n",
        "    #print()\n",
        "    #print(df_normalized)\n",
        "    return df_normalized"
      ],
      "metadata": {
        "id": "5eIp5ou0u0He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def npfy(df,m):\n",
        "    x = []\n",
        "    # row = []\n",
        "    y = []\n",
        "    df_in = df.drop(['Observed'], axis = 1 )\n",
        "    df_in_np = df_in.to_numpy()\n",
        "    for i in range(len(df_in)-m):\n",
        "      # for j in range(0,m+1):\n",
        "      #   row = [df_in_np[i+j] for j in range(0,m+1)]\n",
        "      x.append([df_in_np[i]])\n",
        "      y.append(df['Observed'][i+m])\n",
        "      #row  = []\n",
        "    x_1 = np.array(x)\n",
        "    y_1 = np.array(y)\n",
        "    del x, y, df_in, df_in_np\n",
        "    #print(x1.shape, y1.shape)\n",
        "    return x_1, y_1"
      ],
      "metadata": {
        "id": "AkbrCbWK909-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        units=hp.Int('units', min_value=32, max_value=96, step=32),\n",
        "        activation=hp.Choice('activation', values=['tanh', 'relu', 'linear']),\n",
        "        return_sequences=True,\n",
        "        input_shape=(1, x_train.shape[2]),\n",
        "        dropout=hp.Float('dropout', min_value=0.0, max_value=0.4, step=0.1),\n",
        "        recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.0, max_value=0.4, step=0.1)\n",
        "    ))\n",
        "    for i in range(hp.Int('num_layers', min_value=1, max_value=2)):\n",
        "        model.add(LSTM(\n",
        "            units=hp.Int('units', min_value=32, max_value=96, step=32),\n",
        "            activation=hp.Choice('activation', values=['tanh', 'relu','linear']),\n",
        "            return_sequences=True if i < hp.Int('num_layers', min_value=1, max_value=2) - 1 else False,\n",
        "            dropout=hp.Float('dropout', min_value=0.0, max_value=0.4, step=0.1),\n",
        "            recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.0, max_value=0.4, step=0.1)\n",
        "        ))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(\n",
        "        loss=MeanSquaredError(),\n",
        "        optimizer=Adam(learning_rate=0.01), #hp.Float('learning_rate', min_value=1e-3, max_value=1e-2, sampling='log')\n",
        "        metrics=[RootMeanSquaredError()]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "cmZKsKBXvcpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(all_files)):\n",
        "    basin_name = all_files[i][:-14]                                   # Select one basin as test\n",
        "    raw_df = pd.read_csv(os.path.join(in_path,all_files[i]))\n",
        "    #print(un_df)\n",
        "    df_train, df_val, df_test = get_data(all_files[i],raw_df)\n",
        "    train_start, train_end, val_start, val_end, test_start, test_end = get_train_val_test2(all_files[i])\n",
        "    #print(df)\n",
        "    n = 0\n",
        "    out_path = '/content/drive/MyDrive/2_2/04_LSTM_HPtuning/'\n",
        "    if not os.path.exists(out_path):\n",
        "      os.makedirs(out_path)\n",
        "\n",
        "\n",
        "    output.clear()\n",
        "    ###############\n",
        "    x1,y1 = npfy(df_train,n)\n",
        "    x2,y2 = npfy(df_val,n)\n",
        "    x3,y3 = npfy(df_test,n)\n",
        "    #print(x1.shape, y1.shape, x2.shape, y2.shape, x3.shape, y3.shape)\n",
        "\n",
        "    #a = int(input('Check:'))\n",
        "    ###############\n",
        "    #Training, validation and testing data\n",
        "    x_train, y_train = x1, y1\n",
        "    x_val, y_val = x2, y2\n",
        "    x_test, y_test = x3, y3\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_val = x_val.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    y_train = y_train.astype('float32')\n",
        "    y_val = y_val.astype('float32')\n",
        "    y_test = y_test.astype('float32')\n",
        "    del x1, y1, x2, y2, x3, y3\n",
        "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n",
        "    #a = int(input('Check:'))\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize the Bayesian Optimization tuner\n",
        "    tuner = kt.BayesianOptimization(\n",
        "        build_model,\n",
        "        objective='val_loss',\n",
        "        max_trials=10,  # Number of hyperparameter combinations to try\n",
        "        executions_per_trial=2,  # Number of models to train per hyperparameter combination\n",
        "        directory=out_path,  # Directory to save results\n",
        "        project_name=f'{basin_name}_bayesian'  # Project name\n",
        "    )\n",
        "\n",
        "    # Perform the hyperparameter search\n",
        "    tuner.search(\n",
        "        x_train, y_train,\n",
        "        epochs=100,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose = 1)]  # Early stopping to prevent overfitting\n",
        "    )\n",
        "\n",
        "    # Retrieve the best hyperparameters\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    print(\"Best Hyperparameters:\", best_hps.values)\n",
        "\n",
        "\n",
        "    best_hps_dict = best_hps.values\n",
        "\n",
        "    # Convert the hyperparameters dictionary to a DataFrame\n",
        "    best_hps_df = pd.DataFrame([best_hps_dict])\n",
        "\n",
        "    # Save to CSV\n",
        "    best_hps_df.to_csv(os.path.join(out_path, basin_name  + '_best_hp.csv'), index=False)\n",
        "\n",
        "    # # Build the best model\n",
        "    # best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    # # Train the best model\n",
        "    # best_model.fit(\n",
        "    #     x_train, y_train,\n",
        "    #     epochs=50,\n",
        "    #     validation_data=(x_val, y_val),\n",
        "    #     callbacks=[EarlyStopping(monitor='val_loss', patience=5)]\n",
        "    # )\n",
        "\n",
        "    # # Evaluate the best model on the test set\n",
        "    # test_loss = best_model.evaluate(x_test, y_test)\n",
        "    # print(\"Test Loss:\", test_loss)\n",
        "\n",
        "    #a = int(input('Check:'))\n",
        "\n",
        "    # out_model_path = os.path.join(out_path, 'Model')\n",
        "    # out_plot_path = os.path.join(out_path, 'Plot')\n",
        "    # out_Q_path = os.path.join(out_path, 'Predicted')\n",
        "    # if not os.path.exists(out_model_path):\n",
        "    #   os.makedirs(out_model_path)\n",
        "    # if not os.path.exists(out_plot_path):\n",
        "    #   os.makedirs(out_plot_path)\n",
        "    # if not os.path.exists(out_Q_path):\n",
        "    #   os.makedirs(out_Q_path)\n",
        "\n",
        "    # df_summary = pd.DataFrame(columns=['Basin','Days','Dropout','Cells','NSE_tr','NSE_vl','NSE_ts','KGE_tr','KGE_vl','KGE_ts','r_tr','r_vl','r_ts','alpha_tr','alpha_vl','alpha_ts','beta_tr','beta_vl','beta_ts' ])\n",
        "    # loc = 0\n",
        "    # for n in n_days:\n",
        "    #     for dropout in drop_rate:\n",
        "    #         for cell in cells:\n",
        "    #             output.clear()\n",
        "    #             suffix =  '_d' + str(f'{dropout:.3f}') + '_D' + str(f'{n:01d}') #+ '_c' + str(f'{cell:2d}')\n",
        "    #             out_model = os.path.join(out_model_path, basin_name + suffix + '.keras')\n",
        "    #             print(out_model)\n",
        "    #             ###############\n",
        "    #             x1,y1 = npfy(df_train,n)\n",
        "    #             x2,y2 = npfy(df_val,n)\n",
        "    #             x3,y3 = npfy(df_test,n)\n",
        "    #             #print(x1.shape, y1.shape, x2.shape, y2.shape, x3.shape, y3.shape)\n",
        "\n",
        "    #             #a = int(input('Check:'))\n",
        "    #             ###############\n",
        "    #             #Training, validation and testing data\n",
        "    #             x_train, y_train = x1, y1\n",
        "    #             x_val, y_val = x2, y2\n",
        "    #             x_test, y_test = x3, y3\n",
        "    #             x_train = x_train.astype('float32')\n",
        "    #             x_val = x_val.astype('float32')\n",
        "    #             x_test = x_test.astype('float32')\n",
        "    #             y_train = y_train.astype('float32')\n",
        "    #             y_val = y_val.astype('float32')\n",
        "    #             y_test = y_test.astype('float32')\n",
        "    #             del x1, y1, x2, y2, x3, y3\n",
        "    #             print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n",
        "    #             #a = int(input('Check:'))\n",
        "    #             ###############\n",
        "    #             #model setup\n",
        "    #             model = Sequential()\n",
        "    #             model.add(LSTM(cell, activation = None, return_sequences = True, input_shape = (1,x_train.shape[2]), dropout = dropout))\n",
        "    #             model.add(LSTM(cell, activation = None, return_sequences = True,))\n",
        "    #             model.add(LSTM(cell, activation = None))\n",
        "    #             model.add(Dense(1))\n",
        "    #             model.compile(loss = MeanSquaredError(), optimizer = Adam(learning_rate = l_rate), metrics = [RootMeanSquaredError()])\n",
        "    #             print(model.summary())\n",
        "\n",
        "    #             ###############\n",
        "    #             #Callbacks\n",
        "    #             earlyStop = EarlyStopping(monitor='val_loss',\n",
        "    #                 patience = 8,\n",
        "    #                 verbose = 1,\n",
        "    #                 mode ='min',\n",
        "    #                 restore_best_weights = True,\n",
        "    #                 start_from_epoch = 15,\n",
        "    #             )\n",
        "    #             checkpoint = ModelCheckpoint(out_model,\n",
        "    #                 verbose = 1,\n",
        "    #                 monitor = 'val_loss',\n",
        "    #                 save_best_only = True,\n",
        "    #                 mode = 'min',\n",
        "    #             )\n",
        "    #             # reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "    #             #     factor = 0.2,\n",
        "    #             #     patience = 10,\n",
        "    #             #     verbose = 1,\n",
        "    #             #     mode = 'min',\n",
        "    #             #     min_delta = 0.001,\n",
        "    #             #     cooldown = 0,\n",
        "    #             #     min_lr = 0.001,\n",
        "    #             # )\n",
        "    #             ###############\n",
        "    #             #Train model\n",
        "    #             training = model.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 50 , callbacks = [ checkpoint ,  earlyStop ])   #reduce_lr ,\n",
        "\n",
        "    #             ###############\n",
        "    #             #Load saved model\n",
        "    #             model.load_weights(filepath = out_model)\n",
        "\n",
        "    #             ###############\n",
        "    #             #Make predictions\n",
        "    #             train_predictions = model.predict(x_train).flatten()\n",
        "    #             train_results = pd.DataFrame(data={'Train Predictions':train_predictions, 'Actuals':y_train})\n",
        "    #             val_predictions = model.predict(x_val).flatten()\n",
        "    #             val_results = pd.DataFrame(data={'Val Predictions':val_predictions, 'Actuals':y_val})\n",
        "    #             test_predictions = model.predict(x_test).flatten()\n",
        "    #             test_results = pd.DataFrame(data={'Test Predictions':test_predictions, 'Actuals':y_test})\n",
        "\n",
        "    #             ###############\n",
        "    #             #Save Predictions\n",
        "    #             df_out = raw_df[['time','Observed']]\n",
        "    #             df_out['Predicted'] = 'None'\n",
        "    #             df_out['Check'] = 'None'\n",
        "    #             df_out['Predicted'][train_start+n:train_end],df_out['Check'][train_start+n:train_end] = train_results['Train Predictions'],train_results['Actuals']\n",
        "    #             df_out['Predicted'][val_start+n:val_end],df_out['Check'][val_start+n:val_end] = val_results['Val Predictions'],val_results['Actuals']\n",
        "    #             df_out['Predicted'][test_start+n:test_end],df_out['Check'][test_start+n:test_end] = test_results['Test Predictions'],test_results['Actuals']\n",
        "    #             df_out = df_out[['Observed','Check','Predicted']]\n",
        "    #             #print(df_out)\n",
        "    #             out_file = os.path.join(out_Q_path, basin_name + suffix + '.csv')\n",
        "    #             df_out.to_csv(out_file)\n",
        "    #             del df_out\n",
        "\n",
        "    #             ##################\n",
        "    #             #Calculate and record statistical performance\n",
        "    #             simulations_tr = train_results['Train Predictions']\n",
        "    #             actuals_tr = train_results['Actuals']\n",
        "    #             nse_tr = he.evaluator(he.nse, simulations_tr, actuals_tr)\n",
        "    #             kge_tr, r_tr, alpha_tr, beta_tr = he.evaluator(he.kge, simulations_tr, actuals_tr)\n",
        "\n",
        "    #             simulations_vl = val_results['Val Predictions']\n",
        "    #             actuals_vl = val_results['Actuals']\n",
        "    #             nse_vl = he.evaluator(he.nse, simulations_vl, actuals_vl)\n",
        "    #             kge_vl, r_vl, alpha_vl, beta_vl = he.evaluator(he.kge, simulations_vl, actuals_vl)\n",
        "\n",
        "    #             simulations_ts = test_results['Test Predictions']\n",
        "    #             actuals_ts = test_results['Actuals']\n",
        "    #             nse_ts = he.evaluator(he.nse, simulations_ts, actuals_ts)\n",
        "    #             kge_ts, r_ts, alpha_ts, beta_ts = he.evaluator(he.kge, simulations_ts, actuals_ts)\n",
        "\n",
        "    #             df_summary.loc[loc] = [basin_name,n, dropout, cell, nse_tr[0], nse_vl[0], nse_ts[0], kge_tr[0], kge_vl[0], kge_ts[0],\n",
        "    #                                   r_tr[0], r_vl[0], r_ts[0], alpha_tr[0], alpha_vl[0], alpha_ts[0], beta_tr[0], beta_vl[0], beta_ts[0] ]\n",
        "    #             loc += 1\n",
        "\n",
        "    #             sim_ts = test_results.dropna()                                                                         #For FDC and required as only test data set contains missing observed values\n",
        "    #             del train_predictions, val_predictions, test_predictions, train_results, val_results, test_results\n",
        "\n",
        "    #             ###############\n",
        "    #             #Hydrograph Plots\n",
        "    #             fig, axes = plt.subplots(1,1, figsize=(15,8))\n",
        "    #             fig.suptitle(f'Training Set {basin_name}_{suffix}')\n",
        "    #             axes.plot(simulations_tr[:1500],label = 'Predicted')\n",
        "    #             axes.plot(actuals_tr[:1500],label = 'Actual')\n",
        "    #             plt.legend()\n",
        "    #             plt.show()\n",
        "    #             out_plot = os.path.join(out_plot_path, basin_name + '_Train' + suffix + '.png' )\n",
        "    #             fig.savefig(out_plot)\n",
        "\n",
        "    #             fig, axes = plt.subplots(1,1, figsize=(15,8))\n",
        "    #             fig.suptitle(f'Validation Set {basin_name}_{suffix}')\n",
        "    #             axes.plot(simulations_vl[:760],label = 'Predicted')\n",
        "    #             axes.plot(actuals_vl[:760],label = 'Actual')\n",
        "    #             plt.legend()\n",
        "    #             plt.show()\n",
        "    #             out_plot = os.path.join(out_plot_path, basin_name + '_Val' + suffix + '.png' )\n",
        "    #             fig.savefig(out_plot)\n",
        "\n",
        "    #             fig, axes = plt.subplots(1,1, figsize=(15,8))\n",
        "    #             fig.suptitle(f'Testing Set {basin_name}_{suffix}')\n",
        "    #             axes.plot(simulations_ts[:760],label = 'Predicted')\n",
        "    #             axes.plot(actuals_ts[:760],label = 'Actual')\n",
        "    #             plt.legend()\n",
        "    #             plt.show()\n",
        "    #             out_plot = os.path.join(out_plot_path, basin_name + '_Test' + suffix + '.png' )\n",
        "    #             fig.savefig(out_plot)\n",
        "    #             print()\n",
        "\n",
        "    #             #Training History\n",
        "    #             fig, axes = plt.subplots(2,1, figsize=(14, 8))\n",
        "    #             fig.suptitle(f'Training History {basin_name}_{suffix}')\n",
        "    #             axes[0].plot(training.history[\"loss\"], color=\"#1f77b4\", label=\"Training Loss\")\n",
        "    #             axes[0].plot(training.history[\"val_loss\"], linestyle=\":\", marker=\"o\", markersize=3, color=\"#1f77b4\", label=\"Validation Loss\")\n",
        "    #             axes[0].set_ylabel(\"Loss\")\n",
        "    #             axes[0].legend()\n",
        "    #             axes[1].plot(training.history[\"root_mean_squared_error\"], color=\"#ff7f0e\", label=\"MAE\")\n",
        "    #             axes[1].plot(training.history[\"val_root_mean_squared_error\"], linestyle=\":\", marker=\"o\", markersize=3, color=\"#ff7f0e\", label=\"Validation MAE\")\n",
        "    #             axes[1].legend()\n",
        "    #             axes[1].set_ylabel(\"Mean Absolute Error\")\n",
        "    #             axes[1].set_xticks(range(1, len(training.epoch)+1, 4))\n",
        "    #             axes[1].set_xticklabels(range(1, len(training.epoch)+1, 4))\n",
        "    #             plt.legend()\n",
        "    #             plt.show()\n",
        "    #             out_plot = os.path.join(out_plot_path, basin_name + '_History' + suffix + '.png' )\n",
        "    #             fig.savefig(out_plot)\n",
        "    #             print()\n",
        "\n",
        "    #             #Observed vs Predicted\n",
        "    #             fig, axes = plt.subplots(1,3, figsize=(30, 10))\n",
        "    #             fig.suptitle(f'Observed vs Predicted flow at {basin_name}_{suffix}')\n",
        "    #             axes[0].plot(simulations_tr, actuals_tr, \"o\", alpha=0.4)\n",
        "    #             if simulations_tr.max() > actuals_tr.max():\n",
        "    #               max = simulations_tr.max()\n",
        "    #             else:\n",
        "    #               max = actuals_tr.max()\n",
        "    #             axes[0].plot([0,max], [0,max], \"k--\", alpha=0.75)\n",
        "    #             axes[0].set_xlabel(\"Predicted\")\n",
        "    #             axes[0].set_ylabel(\"Observed\")\n",
        "    #             axes[0].set_title('Train')\n",
        "\n",
        "    #             axes[1].plot(simulations_vl, actuals_vl, \"o\", alpha=0.4)\n",
        "    #             if simulations_vl.max() > actuals_vl.max():\n",
        "    #               max = simulations_vl.max()\n",
        "    #             else:\n",
        "    #               max = actuals_vl.max()\n",
        "    #             axes[1].plot([0,max], [0,max], \"k--\", alpha=0.75)\n",
        "    #             axes[1].set_xlabel(\"Predicted\")\n",
        "    #             axes[1].set_ylabel(\"Observed\")\n",
        "    #             axes[1].set_title('Validation')\n",
        "\n",
        "    #             axes[2].plot(simulations_ts, actuals_ts, \"o\", alpha=0.4)\n",
        "    #             if simulations_ts.max() > actuals_ts.max():\n",
        "    #               max = simulations_ts.max()\n",
        "    #             else:\n",
        "    #               max = actuals_ts.max()\n",
        "    #             axes[2].plot([0,max], [0,max], \"k--\", alpha=0.75)\n",
        "    #             axes[2].set_xlabel(\"Predicted\")\n",
        "    #             axes[2].set_ylabel(\"Observed\")\n",
        "    #             axes[2].set_title('Test')\n",
        "    #             plt.legend()\n",
        "    #             plt.show()\n",
        "    #             out_plot = os.path.join(out_plot_path, basin_name + '_OvP' + suffix + '.png' )\n",
        "    #             fig.savefig(out_plot)\n",
        "\n",
        "    #             ###############\n",
        "    #             #FDC\n",
        "    #             sort_sim_tr = np.sort(simulations_tr)[::-1]\n",
        "    #             sort_act_tr = np.sort(actuals_tr)[::-1]\n",
        "    #             sort_sim_vl = np.sort(simulations_vl)[::-1]\n",
        "    #             sort_act_vl = np.sort(actuals_vl)[::-1]\n",
        "    #             sort_sim_ts = np.sort(sim_ts['Test Predictions'])[::-1]\n",
        "    #             sort_act_ts = np.sort(sim_ts['Actuals'])[::-1]\n",
        "    #             exceedence_tr = np.arange(1.,len(sort_sim_tr)+1) / len(sort_sim_tr)\n",
        "    #             exceedence_vl = np.arange(1.,len(sort_sim_vl)+1) / len(sort_sim_vl)\n",
        "    #             exceedence_ts = np.arange(1.,len(sort_sim_ts)+1) / len(sort_sim_ts)\n",
        "\n",
        "    #             fig, axes = plt.subplots(1,3, figsize=(19,5))\n",
        "    #             fig.suptitle(f'Flow Duration Curve for {basin_name}_{suffix}')\n",
        "    #             axes[0].plot(100*exceedence_tr,sort_sim_tr,label = 'Predicted')\n",
        "    #             axes[0].plot(100*exceedence_tr,sort_act_tr,label = 'Actual')\n",
        "    #             axes[0].set_xlabel(\"Exceedence [%]\")\n",
        "    #             axes[0].set_ylabel(\"Min-Max Normalized Flow\")\n",
        "    #             axes[0].set_title('Train',y = -.20)\n",
        "    #             axes[1].plot(100*exceedence_vl,sort_sim_vl,label = 'Predicted')\n",
        "    #             axes[1].plot(100*exceedence_vl,sort_act_vl,label = 'Actual')\n",
        "    #             axes[1].set_xlabel(\"Exceedence [%]\")\n",
        "    #             axes[1].set_ylabel(\"Min-Max Normalized Flow\")\n",
        "    #             axes[1].set_title('Validation',y = -.20)\n",
        "    #             axes[2].plot(100*exceedence_ts,sort_sim_ts,label = 'Predicted')\n",
        "    #             axes[2].plot(100*exceedence_ts,sort_act_ts,label = 'Actual')\n",
        "    #             axes[2].set_xlabel(\"Exceedence [%]\")\n",
        "    #             axes[2].set_ylabel(\"Min-Max Normalized Flow\")\n",
        "    #             axes[2].set_title('Test',y = -.20)\n",
        "    #             plt.legend()\n",
        "    #             plt.show()\n",
        "    #             out_plot = os.path.join(out_plot_path, basin_name + '_FDC' + suffix + '.png' )\n",
        "    #             fig.savefig(out_plot)\n",
        "\n",
        "    #             del fig, axes, sort_sim_tr, sort_act_tr, sort_sim_vl, sort_act_vl, sort_sim_ts, sort_act_ts, exceedence_tr, exceedence_vl, exceedence_ts\n",
        "\n",
        "    #             #####################\n",
        "    #             keras.backend.clear_session()\n",
        "    #             #####################\n",
        "\n",
        "    # df_summary.to_csv(os.path.join(out_path, basin_name  + '_Summary.csv'), index = False)\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "KKWJuQGLgX_J",
        "outputId": "5509e50f-ce9f-42f5-9d59-c464c87bccf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 01m 31s]\n",
            "val_loss: 1741.927490234375\n",
            "\n",
            "Best val_loss So Far: 1367.72998046875\n",
            "Total elapsed time: 00h 13m 05s\n",
            "Best Hyperparameters: {'units': 32, 'activation': 'tanh', 'dropout': 0.1, 'recurrent_dropout': 0.2, 'num_layers': 1}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'train_s' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cf89254de2d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mraw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(un_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_val_test2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c91318dba07b>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(b_name, r_df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't2m_mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'src_mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'snowc_mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tp_sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ssro_sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e_sum'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'sf_sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Observed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_val_test2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(train_start, train_end, val_start, val_end, test_start, test_end)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bedfe19e4394>\u001b[0m in \u001b[0;36mget_train_val_test2\u001b[0;34m(basin_name)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtest_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavailable_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#print(train_s,train_e,val_s, val_e, test_s, test_e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'train_s' where it is not associated with a value"
          ]
        }
      ]
    }
  ]
}